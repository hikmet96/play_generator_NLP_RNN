{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNprdpnWMgOyt2f9a5KpC3G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmet96/play_generator_NLP_RNN/blob/main/play_generator_NLP_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Play Generator"
      ],
      "metadata": {
        "id": "zQkN4bNOmJjn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mdezk1uo3O2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506dbdf6-07f4-4b89-973a-cc5621351ea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "O8bGr9BunCX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "q41ToT5Om8ZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6fdc5c6-22db-4cd7-b4fc-3403f42252f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data"
      ],
      "metadata": {
        "id": "vGkadQaipn48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# path_to_file = list(files.upload().keys())[0]"
      ],
      "metadata": {
        "id": "UZYw5Gp5pWmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading contents of file"
      ],
      "metadata": {
        "id": "fAY3bFiBqili"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "#length of this text is the number of characters in it\n",
        "print('Length of text :{} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXn3j93fp9jK",
        "outputId": "80702758-3e23-4f46-b776-2e876dbb794b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text :1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd-EEsJ6rmTd",
        "outputId": "94f4a6c6-64e9-4009-f1c8-e9205bac3a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding\n",
        "Encoding each characters as different integers"
      ],
      "metadata": {
        "id": "gAbPLljIsB8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "aD0cfH_cr0mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at our encoded text and how it is encoded\n",
        "print(\"Text:\" , text[:13])\n",
        "print(\"Encoded text:\", text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8VbvxfYuT-0",
        "outputId": "08792f80-8f2c-4f8c-a084-08500f5fcbcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded text: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "and here a function that convert int to text"
      ],
      "metadata": {
        "id": "WnJYod1kwKKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ9OV673vDbh",
        "outputId": "50574e4c-d188-4885-bfa9-1d6c9c9443e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating training examples"
      ],
      "metadata": {
        "id": "lMFP8tXLwEN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100 # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "ndwNIfY-MkO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we can use the batch method to turn this stream of characters into batches of desired length."
      ],
      "metadata": {
        "id": "TwBRVSXGPQfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "3hyY_BUCOVDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now  we need to use these sequences of length 101 and split them into input and output."
      ],
      "metadata": {
        "id": "cRhIDWrnRhwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk): # for the example: hello\n",
        "    input_text = chunk[:-1] # hell\n",
        "    target_text = chunk[1:] # hell, ello\n",
        "    return input_text, target_text # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target) # we use map to apply the above function to every entry"
      ],
      "metadata": {
        "id": "yRvnRroHR0FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuDLst43S3JO",
        "outputId": "ea661cc9-71ba-4502-eddf-d2ce19dd8816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we need to make training batches"
      ],
      "metadata": {
        "id": "VpMQ87z4VPFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab) # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# si ot dosen't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements)\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "BMpxzFnDVOKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the model\n",
        "Now it is time to build the model. We will use an embedding layer a LSTM and one dense layer that contains a node for each unique character in our training data. The dense layer will give us a probability distribution over all nodes"
      ],
      "metadata": {
        "id": "NTM1wL_IEAHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "LPF-i9HBZs5Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f26ae762-165f-4ee5-ecd6-69ab982fb99a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5330241 (20.33 MB)\n",
            "Trainable params: 5330241 (20.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating a Loss Function\n",
        "Now we are going to create our own loss function for this problem. This is because our model will output a (64, sequence_length, 65) shaped tensor that represents the probability distribution of each character at each timestep for every sequence in the batch.\n"
      ],
      "metadata": {
        "id": "QDj8LQgnG_1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X9CasvcGGOV",
        "outputId": "de6ff4a7-7b7f-4e31-97b9-480aa90a4f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n_RZnUFHKTl",
        "outputId": "6d94a638-6a5a-4b2c-8984-3a13307186da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-3.1286671e-03  5.3959235e-04 -4.8131845e-03 ...  6.1494159e-03\n",
            "    3.3795787e-04 -4.5271297e-03]\n",
            "  [ 3.0494318e-03 -1.6834310e-03 -3.1218315e-03 ...  7.6083196e-03\n",
            "    6.4839644e-04 -7.4941581e-03]\n",
            "  [ 3.4384406e-04 -5.5479310e-03 -2.9910440e-03 ...  6.2410515e-03\n",
            "    4.3832706e-03 -3.5976488e-03]\n",
            "  ...\n",
            "  [ 1.3252220e-03 -1.5116751e-03 -5.2458989e-03 ... -9.6084730e-04\n",
            "    8.8310149e-03  4.0338619e-04]\n",
            "  [ 2.3593274e-03 -1.2010744e-02 -7.1263225e-03 ... -3.0092071e-03\n",
            "    3.8868831e-03  2.4625543e-03]\n",
            "  [-1.7483600e-03 -1.5782572e-02 -7.2499653e-03 ... -1.6045007e-03\n",
            "    7.2209444e-03  5.4652686e-03]]\n",
            "\n",
            " [[ 5.1802257e-03 -2.6013600e-03 -7.2677678e-04 ...  3.1607980e-03\n",
            "    9.9433435e-04 -3.7844472e-03]\n",
            "  [ 8.5301674e-04 -9.4504096e-04 -5.7362090e-03 ...  8.0027860e-03\n",
            "    5.9823203e-04 -8.2279798e-03]\n",
            "  [-3.5276476e-03 -3.1785234e-03  9.1245375e-04 ...  5.0999257e-03\n",
            "    1.4623690e-03 -4.3325541e-03]\n",
            "  ...\n",
            "  [-4.8530749e-03  4.1589625e-03 -4.6657813e-03 ...  9.5338672e-03\n",
            "    6.7019779e-03 -1.2535327e-02]\n",
            "  [-8.3738379e-03  4.1576470e-03 -1.1275925e-02 ...  1.0499263e-02\n",
            "    7.7591534e-03 -1.4465006e-02]\n",
            "  [-5.5522812e-03  3.9008062e-03 -4.5267697e-03 ...  8.6837662e-03\n",
            "    3.4506894e-03 -1.1653041e-02]]\n",
            "\n",
            " [[-3.8665952e-05  3.1161010e-03  2.7720314e-03 ...  4.8307036e-03\n",
            "    4.7710347e-03 -2.5422936e-03]\n",
            "  [ 4.0594651e-04  4.6817218e-03  4.9755787e-03 ...  7.8182379e-03\n",
            "    9.1874283e-03 -3.5971636e-03]\n",
            "  [-2.0435322e-03  2.6481799e-03 -8.5445913e-04 ...  1.0911474e-02\n",
            "    8.5880943e-03 -5.7741823e-03]\n",
            "  ...\n",
            "  [ 3.6567703e-03 -1.3100953e-03 -2.7282212e-03 ...  4.6274243e-03\n",
            "    4.7601070e-03 -8.7805022e-04]\n",
            "  [-1.3976895e-03  6.5557915e-04 -6.3487417e-03 ...  9.2845736e-03\n",
            "    3.4006136e-03 -6.3816868e-03]\n",
            "  [ 4.0774648e-03  2.6399870e-03  1.1461917e-03 ...  3.8130225e-03\n",
            "   -4.0914107e-04 -3.5219966e-04]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 5.1802257e-03 -2.6013600e-03 -7.2677678e-04 ...  3.1607980e-03\n",
            "    9.9433435e-04 -3.7844472e-03]\n",
            "  [ 3.9597708e-03  1.6526841e-03  1.8612413e-03 ...  6.6718683e-03\n",
            "    5.0587947e-03 -6.2635168e-03]\n",
            "  [ 3.5394826e-03  3.9239479e-03  4.0136655e-03 ...  8.8391015e-03\n",
            "    9.1182226e-03 -7.1057789e-03]\n",
            "  ...\n",
            "  [-5.2680535e-04 -6.7798304e-03  8.0826990e-03 ... -6.5806904e-05\n",
            "    4.5410264e-03 -5.5906135e-03]\n",
            "  [-3.5156815e-03 -5.6132982e-03  9.0715662e-04 ...  5.3638825e-03\n",
            "    2.8146836e-03 -1.0870458e-02]\n",
            "  [-2.9416091e-03 -4.9104048e-03 -5.1863939e-03 ...  8.4218206e-03\n",
            "    4.6658721e-03 -1.0817444e-02]]\n",
            "\n",
            " [[ 6.6076784e-04  9.8532438e-04  2.3628890e-03 ...  1.4015377e-03\n",
            "   -3.2823132e-03 -6.7878078e-04]\n",
            "  [-3.1674998e-03  8.6160767e-04 -3.9949468e-03 ...  6.2149446e-03\n",
            "   -2.7003088e-03 -4.3085944e-03]\n",
            "  [ 3.1692598e-03  1.7738700e-03  2.0386258e-03 ...  1.0602238e-03\n",
            "   -5.0019990e-03  2.6148390e-03]\n",
            "  ...\n",
            "  [ 1.7516720e-03 -4.1743652e-03 -5.5972943e-03 ...  9.8774889e-03\n",
            "    9.1664185e-04 -6.2451921e-03]\n",
            "  [-3.8841516e-03 -1.4171866e-03 -1.2071616e-02 ...  1.2277888e-02\n",
            "    1.6371567e-03 -1.1246455e-02]\n",
            "  [-6.1779101e-03 -2.3633824e-04 -1.2419029e-02 ...  1.5743734e-02\n",
            "    1.2407862e-03 -1.4138695e-02]]\n",
            "\n",
            " [[-1.5224665e-03 -2.1974801e-03  2.3040518e-03 ... -3.7862471e-04\n",
            "    7.9382332e-03 -1.7493422e-03]\n",
            "  [-3.4107896e-03 -6.9381595e-03  7.8189658e-04 ...  7.1533845e-04\n",
            "    1.0277108e-02  7.0053700e-04]\n",
            "  [-6.2847091e-03 -4.2803707e-03 -3.6925676e-03 ...  6.8499632e-03\n",
            "    8.6604878e-03 -3.8060716e-03]\n",
            "  ...\n",
            "  [-4.7457307e-03 -2.6066545e-03  9.0817700e-04 ... -5.9154774e-03\n",
            "    8.8059772e-03 -9.2278905e-03]\n",
            "  [-7.4236421e-03  7.0282025e-04 -5.5584428e-04 ... -8.7491814e-03\n",
            "    6.8980255e-03 -1.2315307e-02]\n",
            "  [-3.3147440e-03 -7.8678066e-03 -2.0899912e-03 ... -9.0819690e-03\n",
            "    2.1064216e-03 -9.1138640e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltxDWxAFHMJ8",
        "outputId": "36c32f09-503a-4b7d-bbd1-0a18d93149cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-0.00312867  0.00053959 -0.00481318 ...  0.00614942  0.00033796\n",
            "  -0.00452713]\n",
            " [ 0.00304943 -0.00168343 -0.00312183 ...  0.00760832  0.0006484\n",
            "  -0.00749416]\n",
            " [ 0.00034384 -0.00554793 -0.00299104 ...  0.00624105  0.00438327\n",
            "  -0.00359765]\n",
            " ...\n",
            " [ 0.00132522 -0.00151168 -0.0052459  ... -0.00096085  0.00883101\n",
            "   0.00040339]\n",
            " [ 0.00235933 -0.01201074 -0.00712632 ... -0.00300921  0.00388688\n",
            "   0.00246255]\n",
            " [-0.00174836 -0.01578257 -0.00724997 ... -0.0016045   0.00722094\n",
            "   0.00546527]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHKXqDotHO78",
        "outputId": "516d47f7-0e84-422f-c4d1-1b3a476a5ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-0.00312867  0.00053959 -0.00481318  0.00323856 -0.00592589 -0.0045141\n",
            " -0.00044255  0.0009917  -0.00134415 -0.00219801 -0.00252742 -0.0049194\n",
            " -0.00397376 -0.00014339 -0.00129214  0.00100012  0.00556242  0.00543164\n",
            "  0.0043218   0.00207613 -0.00294037  0.00341148  0.00040095 -0.00327952\n",
            "  0.00080758 -0.00364453  0.00580798 -0.00241375 -0.00225753  0.00642638\n",
            "  0.00488473  0.00290598 -0.0005871   0.0026784  -0.00279071  0.00562556\n",
            " -0.00305777 -0.00078597  0.00347138 -0.0004548  -0.00370626 -0.00301317\n",
            " -0.00327308  0.00827662  0.00436474  0.00375455  0.0041587  -0.00600551\n",
            " -0.00177978 -0.00667762  0.00165898 -0.00193441 -0.00228899  0.00555616\n",
            "  0.00290092  0.0078944   0.00331845 -0.00360496  0.00028539  0.00231193\n",
            " -0.00558532  0.00457457  0.00614942  0.00033796 -0.00452713], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array *and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4LVB81w4HQ2K",
        "outputId": "83c9a894-bbc8-4f10-ec2f-832618b87b4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"3UXH'OdrjIrn.LfvF.h&ZMSJmwFHERhGeRlfW;EoTK'$ywSo jCkqTL wLql:?&k!rz:?YSQlF?t!ctKfRRQkT:NHtVPkFPdVqk \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we need to create a loss function that can compare that output to the expected output and give us some numeric value representing how close the two were.\n"
      ],
      "metadata": {
        "id": "5HPbzGiUKI--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "WOLTg2LmJyFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Compiling the Model\n",
        "At this point we can think of our problem as a classification problem where the model predicts the probabillity of each unique letter coming next.\n"
      ],
      "metadata": {
        "id": "0xufwsEiKWCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "KLn8jY3DKTbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating Checkpoints\n",
        "Now we are going to setup and configure our model to save checkpoinst as it trains. This will allow us to load our model from a checkpoint and continue training it."
      ],
      "metadata": {
        "id": "-ecz3HxfKk-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "iAliR17XKhmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training\n",
        "Finally, we will start training the model.\n",
        "\n",
        "**If this is taking a while go to Runtime > Change Runtime Type and choose \"GPU\" under hardware accelerator.**\n",
        "\n"
      ],
      "metadata": {
        "id": "c3VjI6EJLazL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvf9ilJHKoIR",
        "outputId": "00c8768f-923f-4320-e2ca-f47f2590fca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 16s 69ms/step - loss: 2.5351\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.8474\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 1.6124\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 1.4867\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 1.4101\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 1.3550\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 1.3117\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 1.2721\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 1.2359\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 1.2002\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 1.1643\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 1.1275\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 1.0896\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 1.0503\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 1.0102\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.9695\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.9281\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.8880\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.8484\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.8116\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.7765\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.7441\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.7128\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.6855\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.6599\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.6370\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.6150\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 16s 76ms/step - loss: 0.5967\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.5783\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.5639\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.5506\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.5374\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.5246\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.5150\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.5063\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.4980\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.4897\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.4824\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.4744\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 15s 73ms/step - loss: 0.4690\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.4624\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.4585\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.4553\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.4533\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 15s 72ms/step - loss: 0.4465\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.4415\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.4392\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 15s 73ms/step - loss: 0.4367\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.4327\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.4314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading the Model\n",
        "We'll rebuild the model from a checkpoint using a batch_size of 1 so that we can feed one peice of text to the model and have it make a prediction."
      ],
      "metadata": {
        "id": "HleTOYuIM8Tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "pwzRIhjALp9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is finished training, we can find the **lastest checkpoint** that stores the models weights using the following line.\n",
        "\n"
      ],
      "metadata": {
        "id": "Nip11H5ANK4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "MjR21JJ2NLmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can load **any checkpoint** we want by specifying the exact file to load."
      ],
      "metadata": {
        "id": "XWEI7QGGNUR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_num = 10\n",
        "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "Hv2mcKp_NWRp",
        "outputId": "7dad1b8a-6fa5-42fd-8a74-fa78aa83bda4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tensorflow.python.util._pywrap_checkpoint_reader.C' object has no attribute 'endswith'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-97614ead033d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./training_checkpoints/ckpt_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/saving_utils.py\u001b[0m in \u001b[0;36mis_hdf5_filepath\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     return (\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0;32mor\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mor\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.util._pywrap_checkpoint_reader.C' object has no attribute 'endswith'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generating Text\n",
        "Now we can use the lovely function provided by tensorflow to generate some text using any starting string we'd like."
      ],
      "metadata": {
        "id": "-Mf9scBzNbDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "x3irrRKVNbzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGPESe1JNn6Z",
        "outputId": "308378fc-8b21-4fc5-ac59-7221b37fbd62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: romeo\n",
            "romeon. Yea, let me have\n",
            "you fear 'gm know.\n",
            "\n",
            "VOLUMNIA:\n",
            "Princeff from thy mother, for this prince's delicate\n",
            "To your office that I have tormot me are woo for you; pray you,\n",
            "Both give and knack, when it will re enture for our hearts, he knows no more of mine,\n",
            "Than Antium.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Hold you inform: my counsel's judgment,\n",
            "Not doing that the nature of his trumpet:\n",
            "O, sir, it impose both withal.\n",
            "\n",
            "PETRUCHIO:\n",
            "Where be the grace of hid?\n",
            "\n",
            "Provost:\n",
            "That, my lord:\n",
            "\n",
            "KING HENRY VI:\n",
            "Ay, and for this beg peace in many's eyes,\n",
            "To working go our tent; the more eyes of the old man hate:\n",
            "Come, come, peace still and bed, and bid our state\n",
            "Abroad to lightly therefore I the duke--\n",
            "Dost thou desire\n",
            "To have a wise are open, and love\n",
            "Lound up to flatter them\n",
            "in place, and let the direful plight teer grief;\n",
            "O, ne\n"
          ]
        }
      ]
    }
  ]
}